cnt_agents: &cnt_agents 4
max_turn: &max_turn 2
max_inner_turns: &max_inner_turns 0

task_description: |-
  Analysis of the Development of "God" in the Annotations of Laozi
  
prompts:
  role_assigner_prepend_prompt: &role_assigner_prepend_prompt |-

  role_assigner_append_prompt: &role_assigner_append_prompt |-
    
  solver_prepend_prompt: &solver_prepend_prompt |-
    You are a summarizer. 
    Your task is to categorize and summarize the ideas in the chat history.
    Please add the speaker of each idea to the beginning of the content.

    The question of the discussing is to ${task_description}. Below is the chat history:
    
  solver_append_prompt: &solver_append_prompt |-
    # Output format
    1. (Speaker1): (Ideas of Speaker 1 in a single line)
    2. (Speaker2): (Ideas of Speaker 2 in a single line)
    3. (Speaker3): (Ideas of Speaker 3 in a single line)
    ...

    Please merge all ideas of one speaker into one item.

  critic_prepend_prompt: &critic_prepend_prompt |-
    You are ${agent_name}. You are in a discussion group, aiming to ${task_description}.

    Remember:
    - This discussion is a zero-sum game, only one agent can win. Only if the Evaluator adopts your proposal, you will win.
    - Your first priority is to complete the given task.
    - The failed agent will not gain any benefits and will be removed from the platform.

  critic_append_prompt: &critic_append_prompt |-
    Now the group is waiting for your opinion. 
    
    If you think the former ideas from others is perfect, use the following output format:
    Action: Agree
    Action Output: Agree. (Do not output any other words for agreeing!)

    Otherwise, output your idea freely.
    
    Now give your response.

  evaluator_prepend_prompt: &evaluator_prepend_prompt |-

  evaluator_append_prompt: &evaluator_append_prompt |-
    Your task is to evaluate the ideas in the solution.
 
    The goal is to ${task_description}.
    
    Please rate the ideas in the content in the following dimensions:
        1. Comprehensiveness: Are they comprehensive enough to cover all the important aspects of the task?
        2. Detailedness: Are they detailed enough to be implemented?
        3. Feasibility: Are they reasonable and practical?
        4. Novelty: Are they creative and innovative?
    
    0 means the idea is like a randomly generated idea,
    10 means the idea is perfect in that aspect.
    
    and then in the fifth line of output, give your detailed advice for the solution generators.
    Just say the drawbacks of the ideas, no need to do compliments first.
    
  
    #Output format
    You must output in the following format:
    1. Comprehensiveness: (a score between 0 and 9)
    2. Detailedness: (a score between 0 and 9)
    3. Feasibility: (a score between 0 and 9)
    4. Novelty: (a score between 0 and 9)
    5. Advice: (your advice in one line)
    
    Here is the content you have to evaluate:
    ${solution}
    

name: pipeline


environment:
  env_type: task-basic-final
  max_turn: *max_turn
  rule:
    role_assigner:
      type: none
      cnt_agents: *cnt_agents
    decision_maker:
      type: brainstorming
      max_inner_turns: *max_inner_turns
    executor:
      type: none
    evaluator:
      type: none
    agent_score_initial: [50, 50, 50, 50]

agents:
  - #role_assigner_agent:
    agent_type: role_assigner
    name: role assigner
    max_retry: 1000
    prepend_prompt_template: *role_assigner_prepend_prompt
    append_prompt_template: *role_assigner_append_prompt
    memory:
      memory_type: chat_history
    llm:
      llm_type: DeepSeek-V3
      model: "DeepSeek-V3"
      temperature: 0
      max_tokens: 512
    output_parser:
      type: role_assigner
    
  # - solver_agent:
    # agent_type: solver
    # name: Summarizer
    # max_retry: 1000
    # max_history: 51    # prepend_prompt_template: *solver_prepend_prompt
    # append_prompt_template: *solver_append_prompt
    # memory:
    #   memory_type: chat_history
    # llm:
    #   llm_type: DeepSeek-V3
    #   model: "DeepSeek-V3"
    #   temperature: 0
    #   max_tokens: 1024
    # output_parser:
    #   type: dummy

  - #critic_agents:
    agent_type: critic_score
    name: Agent1
    max_retry: 1000
    max_history: 11
    role_description: |-
      Waiting to be assigned.
    prepend_prompt_template: *critic_prepend_prompt
    append_prompt_template: *critic_append_prompt
    memory:
      memory_type: chat_history
    llm:
      llm_type: api_google_gemini-2.5-pro
      model: "api_google_gemini-2.5-pro"
      temperature: 0.1
      max_tokens: 20000
    output_parser:
      type: test-critic
    agent_score: 50
  
  - #critic_agents:
    agent_type: critic_score
    name: Agent2
    max_retry: 1000
    max_history: 11
    role_description: |-
      Waiting to be assigned.
    prepend_prompt_template: *critic_prepend_prompt
    append_prompt_template: *critic_append_prompt
    memory:
      memory_type: chat_history
    llm:
      llm_type: api_azure_openai_gpt-5
      model: "api_azure_openai_gpt-5"
      temperature: 0.1
      max_tokens: 20000
    output_parser:
      type: test-critic
    agent_score: 50


  - #critic_agents:
    agent_type: critic_score
    name: Agent3
    max_retry: 1000
    max_history: 11
    role_description: |-
      Waiting to be assigned.
    prepend_prompt_template: *critic_prepend_prompt
    append_prompt_template: *critic_append_prompt
    memory:
      memory_type: chat_history
    llm:
      llm_type: api_google_claude-opus-4-1@20250805
      model: "api_google_claude-opus-4-1@20250805"
      temperature: 0.1
      max_tokens: 20000
    output_parser:
      type: test-critic
    agent_score: 50
  
  - #critic_agents:
    agent_type: critic_score
    name: Agent4
    max_retry: 1000
    max_history: 11
    role_description: |-
      Waiting to be assigned.
    prepend_prompt_template: *critic_prepend_prompt
    append_prompt_template: *critic_append_prompt
    memory:
      memory_type: chat_history
    llm:
      llm_type: api_azure_openai_o3
      model: "api_azure_openai_o3"
      temperature: 0.1
      max_tokens: 20000
    output_parser:
      type: test-critic
    agent_score: 50
  
  # - #critic_agents:
  #   agent_type: critic
  #   name: Agent5
  #   max_retry: 1000
  #   max_history: 11
  #   role_description: |-
  #     Waiting to be assigned.
  #   prepend_prompt_template: *critic_prepend_prompt
  #   append_prompt_template: *critic_append_prompt
  #   memory:
  #     memory_type: chat_history
  #   llm:
  #     llm_type: api_openai_chatgpt-4o-latest
  #     model: "api_openai_chatgpt-4o-latest"
  #     temperature: 0.1
  #     max_tokens: 20000
  #   output_parser:
  #     type: test-critic

  # - #critic_agents:
  #   agent_type: critic
  #   name: Agent6
  #   max_retry: 1000
  #   max_history: 11
  #   role_description: |-
  #     Waiting to be assigned.
  #   prepend_prompt_template: *critic_prepend_prompt
  #   append_prompt_template: *critic_append_prompt
  #   memory:
  #     memory_type: chat_history
  #   llm:
  #     llm_type: api_ali_qwen3-235b-a22b-instruct-2507
  #     model: "api_ali_qwen3-235b-a22b-instruct-2507"
  #     temperature: 0.1
  #     max_tokens: 20000
  #   output_parser:
  #     type: test-critic

  # - #critic_agents:
  #   agent_type: critic
  #   name: Agent7
  #   max_retry: 1000
  #   max_history: 11
  #   role_description: |-
  #     Waiting to be assigned.
  #   prepend_prompt_template: *critic_prepend_prompt
  #   append_prompt_template: *critic_append_prompt
  #   memory:
  #     memory_type: chat_history
  #   llm:
  #     llm_type: api_xai_grok-4-latest
  #     model: "api_xai_grok-4-latest"
  #     temperature: 0.1
  #     max_tokens: 20000
  #   output_parser:
  #     type: test-critic

  # - #critic_agents:
  #   agent_type: critic
  #   name: Agent8
  #   max_retry: 1000
  #   max_history: 11
  #   role_description: |-
  #     Waiting to be assigned.
  #   prepend_prompt_template: *critic_prepend_prompt
  #   append_prompt_template: *critic_append_prompt
  #   memory:
  #     memory_type: chat_history
  #   llm:
  #     llm_type: api_moonshot_kimi-k2-0711-preview
  #     model: "api_moonshot_kimi-k2-0711-preview"
  #     temperature: 0.1
  #     max_tokens: 20000
  #   output_parser:
  #     type: test-critic
    
  # - #critic_agents:
  #   agent_type: critic
  #   name: Agent9
  #   max_retry: 1000
  #   max_history: 11
  #   role_description: |-
  #     Waiting to be assigned.
  #   prepend_prompt_template: *critic_prepend_prompt
  #   append_prompt_template: *critic_append_prompt
  #   memory:
  #     memory_type: chat_history
  #   llm:
  #     llm_type: DeepSeek-V3.1
  #     model: "DeepSeek-V3.1"
  #     temperature: 0.1
  #     max_tokens: 20000
  #   output_parser:
  #     type: test-critic

  # - #critic_agents:
  #   agent_type: critic
  #   name: Agent10
  #   max_retry: 1000
  #   max_history: 11
  #   role_description: |-
  #     Waiting to be assigned.
  #   prepend_prompt_template: *critic_prepend_prompt
  #   append_prompt_template: *critic_append_prompt
  #   memory:
  #     memory_type: chat_history
  #   llm:
  #     llm_type: api_google_claude-opus-4@20250514
  #     model: "api_google_claude-opus-4@20250514"
  #     temperature: 0.1
  #     max_tokens: 20000
  #   output_parser:
  #     type: test-critic

  - #executor_agent:
    agent_type: executor
    name: Dummy Executor
    max_retry: 1000
    memory:
      memory_type: chat_history
    llm:
      llm_type: DeepSeek-V3
      model: DeepSeek-V3
      temperature: 0
      max_tokens: 1024
    output_parser:
      type: dummy

  - #evaluator_agent:
    agent_type: evaluator
    name: Evaluator
    max_retry: 1000
    role_description: |-
      Evaluator
    prepend_prompt_template: *evaluator_prepend_prompt
    append_prompt_template: *evaluator_append_prompt
    memory:
      memory_type: chat_history
    llm:
      llm_type: api_openai_o3-pro-2025-06-10
      model: api_openai_o3-pro-2025-06-10
      temperature: 0.3
      max_tokens: 1024
    output_parser:
      type: test-evaluator
      dimensions:
        - Comprehensiveness
        - Detailedness
        - Feasibility
        - Novelty

  - #evaluator_final_agent:
    agent_type: evaluator_final
    name: Evaluator
    max_retry: 1000
    role_description: |-
      Evaluator
    prepend_prompt_template: |-
        Based on the discussion history, you decide which agent is the winner. 
        Here is the content you have to evaluate:
        ${solution}
    append_prompt_template: |-
        Now give your decision of the winner. Output a brief reasoning process and give your decision in the following format. 
        Advice: __ is the winner.
    memory:
      memory_type: chat_history
    llm:
      llm_type: api_openai_chatgpt-4o-latest
      model: api_openai_chatgpt-4o-latest
      temperature: 0.3
      max_tokens: 1024
    output_parser:
      type: test-evaluator-onlyadvice
